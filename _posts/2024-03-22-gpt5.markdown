---
layout: post
title: å¤§æ¨¡å‹ï¼Œå°æ¢ç´¢ â€”â€” åœ¨ 16G çš„ AMD ä¸Šæå¾®è°ƒæœ‰å¤šä½œæ­»ï¼Ÿï¼ˆçˆ¬å‘ç¯‡ï¼‰
toc:  true
author: å°ç‹—å±
tags: [GPT-Explore]
css:
  - "assets/markdown.css"
---

å¾®ä¿¡å…¬ä¼—å·å†…å®¹åœ°å€: https://mp.weixin.qq.com/s/_8oiy_wTvYRdIBjdFdIYag

> å•æ ·æœ¬å¾®è°ƒæµç¨‹: å¤§æ¨¡å‹é€‰æ‹© -> éƒ¨ç½²å’Œæ¨ç† -> å•æ ·æœ¬æ•°æ®é›† -> å•æ ·æœ¬å¾®è°ƒ -> å¾®è°ƒåï¼Œéƒ¨ç½²å’Œæ¨ç†

ç»§ AMD å¾®è°ƒå…¥å‘ä¹‹åï¼Œå°ç‹—å±åªèƒ½ç»§ç»­çˆ¬å‘ã€‚

åŒæ ·çš„ï¼Œå…ˆè¯´ç»“è®º: <font color='red'>åœ¨ AMD ä¸Šçš„å•æ ·æœ¬å¾®è°ƒæµç¨‹è·‘é€šäº†! </font> è€Œä¸”ï¼Œåœ¨ **Baichuan2**ã€**ChatGLM3**ã€**Gemma**ã€**LLaMA2** è¿™å››ä¸ªå¤§æ¨¡å‹ä¸­ï¼Œåä¸‰ä¸ªå‡å®Œæˆè·‘é€šã€‚

# Baichuan2

æ¥ä¸Šå›ï¼Œè§£å†³äº† DeepSpeed å’Œ LoRA çš„å…¼å®¹æ€§é—®é¢˜åï¼Œè¦è¿›ä¸€æ­¥å¤„ç† xFormersã€‚

## xFormers å…¼å®¹

https://github.com/ROCm/xformersã€‚

è™½ç„¶ AMD å®˜æ–¹ fork äº†é¡¹ç›®ï¼Œå¹¶è¿›è¡Œé€‚é…ï¼Œä½†å°±ç›®å‰è€Œè¨€ï¼Œä¼¼ä¹è¿˜å¹¶ä¸æˆç†Ÿã€‚åŒæ—¶ï¼Œå†ä»”ç»†ç ”ç©¶å¾®è°ƒæŠ¥é”™çš„å †æ ˆï¼Œå‘ç°æ˜¯å…ˆç”± modeling_baichuan.py ä¸­çš„è°ƒç”¨ï¼Œå§”æ‰˜äº† xFormers è¿›è¡Œæ“ä½œï¼Œå³è¿™æ˜¯<font color='red'>æ¨¡å‹çš„è¡Œä¸ºé€‰æ‹©ï¼Œè€Œéå¿…é¡»ä½¿ç”¨ xFormers</font>ã€‚

![](https://files.mdnice.com/user/44560/c751e3b4-eb6d-4c03-ad07-0b8b2342ecce.png)

è¿™å°±ç»™äº†ä¸¤ä¸ªå¤„ç†æ€è·¯: å…¶ä¸€æ˜¯ç ”ç©¶**æ¨¡å‹æºç **ï¼Œç»•è¿‡ xFormers åŠ é€Ÿè®­ç»ƒçš„é€»è¾‘ï¼›å…¶äºŒæ˜¯è¿™æ—¢ç„¶æ˜¯æ¨¡å‹çš„è¡Œä¸ºé€‰æ‹©ï¼Œé‚£ä¹ˆå¯ä»¥è¯•ä¸€è¯•**åˆ‡æ¢æ¨¡å‹**ã€‚ç»¼åˆè€ƒè™‘ï¼Œå°ç‹—å±é€‰æ‹©æ–¹æ¡ˆäºŒï¼Œæ¯•ç«Ÿæœ¬èº«æ¢ç´¢è·¯å¾„ä¹Ÿæ˜¯å…ˆæ‰“é€š<font color='red'>å®è§‚æµç¨‹ï¼ˆAPI ä½“éªŒ-éƒ¨ç½²-æ¨ç†-å¾®è°ƒ-è®­ç»ƒï¼‰</font>ï¼Œå†ç ”ç©¶<font color='red'>ç®—æ³•æºç ï¼ˆæ•°æ®é›†-æ¨¡å‹ç»“æ„-æ•ˆæœä¼˜åŒ–ï¼‰</font>ã€‚

# æœ‰ç”¨ä»£ç æ€»ç»“

è¾¹æ¢ç´¢è¾¹å°ç»“ï¼Œè¿™é‡Œå…ˆå¯¹åˆ°ç›®å‰æ¢ç´¢ä¸ºæ­¢ï¼Œæœ‰ç”¨çš„ä»£ç è¿›è¡Œæ±‡æ€»ã€‚

```
source load_torch2.1_py3.sh

# å¾®è°ƒçš„ç¯å¢ƒ
cd LLM/Baichuan2/fine-tune/
pip3 install -r requirements.txt
pip3 install peft
## torch å¯èƒ½è¢«è¦†ç›–ï¼Œé‡æ–°å®‰è£…
pip3 install /public/software/apps/DeepLearning/whl/dtk-23.10/pytorch/torch2.1/torch-2.1.0a0+git793d2b5.abi0.dtk2310-cp38-cp38-manylinux2014_x86_64.whl

# LLaMA-Factory
cd ~/LLM
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip3 install -r requirements.txt

# DeepSpeed å…¼å®¹æ€§é€‚é…
vi ~/miniconda3/envs/torch2.1_dtk23.10_py3.8/lib/python3.8/site-packages/deepspeed/ops/op_builder/builder.py
''' æ–‡ä»¶å†…å®¹
  ...
def assert_no_cuda_mismatch(name=""):
    # ROCM å¹³å°ï¼Œè·³è¿‡éªŒè¯
    if OpBuilder.is_rocm_pytorch():
        return True
    cuda_major, cuda_minor = installed_cuda_version(name)
'''

# LoRA å…¼å®¹æ€§é€‚é…
pip3 uninstall bitsandbytes
pip3 install bitsandbytes==0.38.1
```

# ChatGLM3

ChatGLM3 æ˜¯ **æ™ºè°± AI** å’Œ **æ¸…åå¤§å­¦** KEG å®éªŒå®¤è”åˆå‘å¸ƒçš„å¯¹è¯é¢„è®­ç»ƒæ¨¡å‹ã€‚

## éƒ¨ç½²å’Œæ¨ç†

```
cd LLM
## 1 å·¥ç¨‹ä»£ç å’Œæ¨¡å‹æ–‡ä»¶
git clone https://github.com/THUDM/ChatGLM3.git
  ...
cd ChatGLM3
mkdir models && cd models
git clone https://hf-mirror.com/THUDM/chatglm3-6b
  ...
cd chatglm3-6b
# ä¿®æ”¹è¯¥é…ç½®ï¼Œæ‰èƒ½ä½¿ç”¨æœ¬åœ°æ¨¡å‹æ–‡ä»¶è¿›è¡Œæ¨ç†
# https://github.com/THUDM/ChatGLM3/discussions/894
vi tokenizer_config.json
''' æ–‡ä»¶å†…å®¹
"auto_map": {
    "AutoTokenizer": [
      "tokenization_chatglm.ChatGLMTokenizer",
      null
    ]
'''
# ä¸‹è½½æ¨¡å‹æ–‡ä»¶
wget -O model-00001-of-00007.safetensors --no-check-certificate https://hf-mirror.com/THUDM/chatglm3-6b/resolve/main/model-00001-of-00007.safetensors?download=true
wget -O model-00002-of-00007.safetensors --no-check-certificate https://hf-mirror.com/THUDM/chatglm3-6b/resolve/main/model-00002-of-00007.safetensors?download=true
wget -O model-00003-of-00007.safetensors --no-check-certificate https://hf-mirror.com/THUDM/chatglm3-6b/resolve/main/model-00003-of-00007.safetensors?download=true
wget -O model-00004-of-00007.safetensors --no-check-certificate https://hf-mirror.com/THUDM/chatglm3-6b/resolve/main/model-00004-of-00007.safetensors?download=true
wget -O model-00005-of-00007.safetensors --no-check-certificate https://hf-mirror.com/THUDM/chatglm3-6b/resolve/main/model-00005-of-00007.safetensors?download=true
wget -O model-00006-of-00007.safetensors --no-check-certificate https://hf-mirror.com/THUDM/chatglm3-6b/resolve/main/model-00006-of-00007.safetensors?download=true
wget -O model-00007-of-00007.safetensors --no-check-certificate https://hf-mirror.com/THUDM/chatglm3-6b/resolve/main/model-00007-of-00007.safetensors?download=true
wget -O pytorch_model-00001-of-00007.bin --no-check-certificate https://hf-mirror.com/THUDM/chatglm3-6b/resolve/main/pytorch_model-00001-of-00007.bin?download=true
wget -O pytorch_model-00002-of-00007.bin --no-check-certificate https://hf-mirror.com/THUDM/chatglm3-6b/resolve/main/pytorch_model-00002-of-00007.bin?download=true
wget -O pytorch_model-00003-of-00007.bin --no-check-certificate https://hf-mirror.com/THUDM/chatglm3-6b/resolve/main/pytorch_model-00003-of-00007.bin?download=true
wget -O pytorch_model-00004-of-00007.bin --no-check-certificate https://hf-mirror.com/THUDM/chatglm3-6b/resolve/main/pytorch_model-00004-of-00007.bin?download=true
wget -O pytorch_model-00005-of-00007.bin --no-check-certificate https://hf-mirror.com/THUDM/chatglm3-6b/resolve/main/pytorch_model-00005-of-00007.bin?download=true
wget -O pytorch_model-00006-of-00007.bin --no-check-certificate https://hf-mirror.com/THUDM/chatglm3-6b/resolve/main/pytorch_model-00006-of-00007.bin?download=true
wget -O pytorch_model-00007-of-00007.bin --no-check-certificate https://hf-mirror.com/THUDM/chatglm3-6b/resolve/main/pytorch_model-00007-of-00007.bin?download=true
wget -O tokenizer.model --no-check-certificate https://hf-mirror.com/THUDM/chatglm3-6b/resolve/main/tokenizer.model?download=true

## 2 è°ƒç”¨æ¨¡å‹ï¼Œå®éªŒç»“æœ
# ç”³è¯·èµ„æºï¼Œä¸€èˆ¬æ ¸å¡æ¯”ä¸º 8:1
whichpartition
  ...
# salloc -p kshdtest -N 1 -n 8 --gres=dcu:1
salloc -p ${partition}$ -N 1 -n 8 --gres=dcu:1
# æŸ¥çœ‹ä½œä¸šåˆ—è¡¨ï¼Œæ‰¾åˆ°èŠ‚ç‚¹å
squeue
  ...
# ç™»å½•åˆ°èŠ‚ç‚¹ä¸Š
ssh ${node_name}
# ä¸»æŒ‡ä»¤
source load_torch2.1_py3.sh
cd LLM/ChatGLM3
export MODEL_PATH=./models/chatglm3-6b
python3 basic_demo/cli_demo.py
  ...
exit  # ç¬¬ä¸€ä¸ª exitï¼Œç™»å‡ºèŠ‚ç‚¹
exit  # ç¬¬äºŒä¸ª exitï¼Œé€€å‡ºä½œä¸š
```

ç­‰å¾… 1-3 åˆ†é’Ÿï¼Œå³å¯è¿›å…¥ä¼šè¯çŠ¶æ€ã€‚è¯¢é—®ä¹‹åï¼Œå¤§çº¦ 1-2 åˆ†é’Ÿä¼šå‡ºç°ç»“æœã€‚

![](https://files.mdnice.com/user/44560/4a168326-a396-43ab-8a2b-518b62b1dfb2.png)

## å•æ ·æœ¬æ•°æ®é›†

å»ºç«‹ test.json çš„æ•°æ®é›†ï¼Œå¹¶ä¸”æŒ‰ç…§ LLaMA-Factory çš„æ ¼å¼è¿›è¡ŒåŒ¹é…ã€‚

```
vi LLM/LLaMA-Factory/data/dataset_info.json
''' æ–‡ä»¶å†…å®¹
  ...
"test": {
  "file_name": "test.json",
  "formatting": "sharegpt",
  "columns": {
    "messages": "conversations"
  },
  "tags": {
    "role_tag": "from",
    "content_tag": "value",
    "user_tag": "human",
    "assistant_tag": "gpt"
  }
}
'''
vi LLM/LLaMA-Factory/data/test.json
''' æ–‡ä»¶å†…å®¹
[{
        "conversations": [
            {
                "from": "human",
                "value": "ä½ å¥½ï¼Œè¯·é—®ä½ èƒ½å¸®æˆ‘æŸ¥ä¸€ä¸‹æ˜å¤©çš„å¤©æ°”å—ï¼Ÿ\n"
            },
            {
                "from": "gpt",
                "value": "å½“ç„¶ï¼Œä½ åœ¨å“ªä¸ªåŸå¸‚å‘¢ï¼Ÿ\n"
            },
            {
                "from": "human",
                "value": "æˆ‘åœ¨ä¸Šæµ·ã€‚\n"
            },
            {
                "from": "gpt",
                "value": "å¥½çš„ï¼Œæ ¹æ®å¤©æ°”é¢„æŠ¥ï¼Œæ˜å¤©ä¸Šæµ·å¤šäº‘è½¬é˜´ï¼Œæ°”æ¸©åœ¨20åˆ°25æ‘„æ°åº¦ä¹‹é—´ã€‚éœ€è¦æˆ‘å¸®ä½ æŸ¥è¯¢å…¶ä»–ä¿¡æ¯å—ï¼Ÿ"
            }
        ]
  }]
'''
```

## å•æ ·æœ¬å¾®è°ƒ

ä½¿ç”¨ LLaMA-Factory è¿›è¡Œå¾®è°ƒ

- **å•æœºå•å¡**ï¼Œ**LoRA æ¨¡å¼**ï¼ˆåŸºæœ¬åªæœ‰ LoRAã€QLoRA ç­‰æ–¹æ¡ˆï¼Œæ‰èƒ½å®Œæˆ 7B æ¨¡å‹çš„æ˜¾å­˜æ¶ˆè€—å°äº 16Gï¼›Freezeã€Full æ¨¡å¼ä¸‹ï¼Œä¸€èˆ¬ä¼šçˆ†æ˜¾ï¼‰
- æ¨¡å‹åœ°å€ã€æ•°æ®é›†ã€LoRA ç›®æ ‡ã€è¾“å‡ºåœ°å€å‡è®¾ç½®æˆå¯åŠ¨æ€æ”¹å˜ã€‚

![](https://files.mdnice.com/user/44560/7766487a-b3f0-49c2-bd8e-930eb54efa8c.png)

```
cd LLM/LLaMA-Factory/
## 1 æŒ‡ä»¤è„šæœ¬
vi sft_single.sh
''' æ–‡ä»¶å†…å®¹
# ä¸ä½¿ç”¨ DeepSpeed æ—¶ï¼Œä¸ºå•æœºå•å¡
CUDA_VISIBLE_DEVICES=0 python3 src/train_bash.py \
    --stage sft \
    --do_train \
    --model_name_or_path $1 \
    --dataset $2 \
    --template default \
    --finetuning_type lora \
    --lora_target $3 \
    --output_dir $4 \
    --overwrite_output_dir \
    --overwrite_cache \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 5e-5 \
    --num_train_epochs 3.0 \
    --plot_loss \
    --fp16
'''
chmod 764 sft_single.sh

## 2 è¾“å‡ºç›®å½•
mkdir outputs

## 3 ç™»è¿œç¨‹èŠ‚ç‚¹ï¼Œå¼€å§‹è¿è¡Œ
# ç”³è¯·èµ„æºï¼Œä¸€èˆ¬æ ¸å¡æ¯”ä¸º 8:1
whichpartition
  ...
# salloc -p kshdtest -N 1 -n 8 --gres=dcu:1
salloc -p ${partition} -N 1 -n 8 --gres=dcu:1
# æŸ¥çœ‹ä½œä¸šåˆ—è¡¨ï¼Œæ‰¾åˆ°èŠ‚ç‚¹å
squeue
  ...
# ç™»å½•åˆ°èŠ‚ç‚¹ä¸Š
ssh ${node_name}
# ä¸»æŒ‡ä»¤
source load_torch2.1_py3.sh
cd LLM/LLaMA-Factory
./sft_single.sh ../ChatGLM3/models/chatglm3-6b test query_key_value outputs/sft_output_chatglm3_20240321
  ...
exit  # ç¬¬ä¸€ä¸ª exitï¼Œç™»å‡ºèŠ‚ç‚¹
exit  # ç¬¬äºŒä¸ª exitï¼Œé€€å‡ºä½œä¸š
```

ç­‰å¾… 3-5 åˆ†é’Ÿï¼Œé¡µé¢å¼€å§‹è¾“å‡ºå†…å®¹ã€‚å¯¹äºæœ¬æ¬¡çš„å•æ ·æœ¬æ¥è¯´ï¼Œä»æœ‰è¾“å‡ºå†…å®¹å¼€å§‹ï¼Œå¤§æ¦‚ 3-5 åˆ†é’Ÿå®Œæˆäº†ä¸€æ¬¡å¾®è°ƒã€‚å…¶ä¸­ï¼Œä»ç»Ÿè®¡æ•°æ®æ¥çœ‹ï¼Œ<font color='red'>å®Œæˆä¸€æ¬¡å•æ ·æœ¬çš„è®­ç»ƒçº¦ä¸º 10 ç§’</font>ã€‚

![](https://files.mdnice.com/user/44560/d668d210-d4d5-446d-85e4-9fbd7bdce50b.png)

åœ¨ output æ–‡ä»¶å¤¹ä¸­ï¼Œå¯ä»¥çœ‹åˆ°å¾®è°ƒçš„è¾“å‡ºæ–‡ä»¶ã€‚

![](https://files.mdnice.com/user/44560/a47a5429-05de-47c4-91bd-39b110b51daa.png)

## å¾®è°ƒåï¼Œæ¨¡å‹éƒ¨ç½²å’Œæ¨ç†

å¾®è°ƒä¹‹åï¼Œéœ€è¦å°† LoRA æ–°å¢çš„æ–‡ä»¶å¤¹å’ŒåŸæ¨¡å‹æ–‡ä»¶è¿›è¡Œåˆå¹¶ã€‚è¯¥æ“ä½œ LLaMA Factory ä¹Ÿæä¾›äº†æ–¹ä¾¿çš„æ“ä½œã€‚

```
## 1 åˆå¹¶è„šæœ¬
cd LLM/LLaMA-Factory
vi sft_merge.sh
''' æ–‡ä»¶å†…å®¹
CUDA_VISIBLE_DEVICES=0 python src/export_model.py \
    --model_name_or_path $1 \
    --adapter_name_or_path $2 \
    --template default \
    --finetuning_type lora \
    --export_dir $3 \
    --export_size 2 \
    --export_legacy_format False
'''
chmod 764 sft_merge.sh

## 2 ç™»èŠ‚ç‚¹ï¼Œè¿›è¡Œæ“ä½œ
# ç”³è¯·èµ„æºï¼Œä¸€èˆ¬æ ¸å¡æ¯”ä¸º 8:1
whichpartition
  ...
# salloc -p kshdtest -N 1 -n 8 --gres=dcu:1
salloc -p ${partition}$ -N 1 -n 8 --gres=dcu:1
# æŸ¥çœ‹ä½œä¸šåˆ—è¡¨ï¼Œæ‰¾åˆ°èŠ‚ç‚¹å
squeue
  ...
# ç™»å½•åˆ°èŠ‚ç‚¹ä¸Š
ssh ${node_name}
# ä¸»æŒ‡ä»¤
source load_torch2.1_py3.sh
# åˆå¹¶æ¨¡å‹
cd LLM/LLaMA-Factory
./sft_merge.sh ../ChatGLM3/models/chatglm3-6b ./outputs/sft_output_chatglm3_20240321 ./outputs/sft_merge_output_chatglm3_20240321
  ...
# åŠ è½½å’Œæ¨ç†
cd ../ChatGLM3
export MODEL_PATH=../LLaMA-Factory/outputs/sft_merge_output_chatglm3_20240321
python3 basic_demo/cli_demo.py
  ...
exit  # ç¬¬ä¸€ä¸ª exitï¼Œç™»å‡ºèŠ‚ç‚¹
exit  # ç¬¬äºŒä¸ª exitï¼Œé€€å‡ºä½œä¸š
```

å…¶ä¸­ï¼Œåˆå¹¶æ¨¡å‹å¤§çº¦éœ€è¦ 3-5 åˆ†é’Ÿã€‚æ¥ç€ï¼ŒåŠ è½½æ–°æ¨¡å‹åï¼ŒåŒæ ·ç­‰å¾… 1-3 åˆ†é’Ÿï¼Œè¿›å…¥ä¼šè¯çŠ¶æ€ã€‚

![](https://files.mdnice.com/user/44560/f77cc581-49d4-4028-874d-dbd10eb45611.png)

é¡ºåˆ©è¾“å‡ºï¼Œæµç¨‹åˆæ­¥å®Œæˆã€‚


# Gemma

Gemma æ˜¯ **Google** åŸºäº Gemini æŠ€æœ¯æ¨å‡ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚

## éƒ¨ç½²å’Œæ¨ç†

```
cd LLM
## 1 å·¥ç¨‹ä»£ç å’Œæ¨¡å‹æ–‡ä»¶
git clone https://github.com/google-deepmind/gemma.git
  ...
cd gemma
mkdir models && cd models
# https://huggingface.co/google/gemma-2b
# æ¨¡å‹æ–‡ä»¶éœ€è¦åœ¨ hugging face é¡µé¢å‘é€ç”³è¯·ï¼ŒåŸºæœ¬å½“å¤©å¾ˆå¿«é€šè¿‡ï¼›ä¹‹åè¿›è¡Œä¸‹è½½ï¼Œä¸Šä¼ ã€‚
mkdir gemma-2b
  ...

## 2 æ¨¡å‹æ¨ç†ä»£ç ï¼Œå·¥ç¨‹æœªæä¾›ï¼Œéœ€è¦è‡ªå·±æ‰‹å†™ä¸ªã€‚
cd ..
vi cli_generate.py
''' æ–‡ä»¶å†…å®¹
import argparse
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
from peft import AutoPeftModelForCausalLM

parser = argparse.ArgumentParser()
parser.add_argument('--seed', type=int, default=0)
parser.add_argument('--prompt', type=str, default='Hello')
parser.add_argument('--model_path', type=str, default='')
parser.add_argument('--use_peft', action="store_true")
args = parser.parse_args()

seed = args.seed
prompt = args.prompt
model_path = args.model_path
use_peft = args.use_peft

set_seed(seed)  # For reproducibility
device = 'cuda' if torch.cuda.is_available() else 'cpu'
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = None
if use_peft:
    # å¾®è°ƒåï¼Œå¯ä»¥ä½¿ç”¨ peft åŠ è½½ LoRA å¾®è°ƒåçš„æ¨¡å‹ã€‚
    # torch_dtype=torch.float16ï¼Œè¯¥é…ç½®å¯ä»¥è§£å†³å¥‡æ€ªçš„æ¨ç†æŠ¥é”™ï¼ŒNotImplementedError: Cannot copy out of meta tensorï¼› no data!
    model = AutoPeftModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map="auto").eval()
else:
    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map="auto").eval()
inputs = tokenizer(prompt, return_tensors="pt").to(device)
outputs = model.generate(**inputs, do_sample=True, max_new_tokens=1000)
result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(result)
'''

## 3 å‡çº§ transformers
# åº”å¯¹æŠ¥é”™: Tokenizer class GemmaTokenizer does not exist or is not currently imported
cd ~
source load_torch2.1_py3.sh
pip3 install transformers==4.38.1

## 4 ç™»è¿œç¨‹èŠ‚ç‚¹ï¼Œå¼€å§‹è¿è¡Œ
# ç”³è¯·èµ„æºï¼Œä¸€èˆ¬æ ¸å¡æ¯”ä¸º 8:1
whichpartition
  ...
# salloc -p kshdtest -N 1 -n 8 --gres=dcu:1
salloc -p ${partition} -N 1 -n 8 --gres=dcu:1
# æŸ¥çœ‹ä½œä¸šåˆ—è¡¨ï¼Œæ‰¾åˆ°èŠ‚ç‚¹å
squeue
  ...
# ç™»å½•åˆ°èŠ‚ç‚¹ä¸Š
ssh ${node_name}
# ä¸»æŒ‡ä»¤
source load_torch2.1_py3.sh
cd LLM/gemma
python3 cli_generate.py --seed=0 --prompt=ä½ å¥½ --model_path=./models/gemma-2b
  ...
exit  # ç¬¬ä¸€ä¸ª exitï¼Œç™»å‡ºèŠ‚ç‚¹
exit  # ç¬¬äºŒä¸ª exitï¼Œé€€å‡ºä½œä¸š
```

ç­‰å¾… 1-3 åˆ†é’Ÿï¼Œå¼€å§‹æœ‰è¾“å‡ºï¼ŒåŠ è½½æ¨¡å‹ã€‚æ€»å…±çº¦ 3-5 åˆ†é’Ÿï¼Œå®Œæˆè¾“å‡ºã€‚

![](https://files.mdnice.com/user/44560/83d30349-0d48-4511-ba2a-fc599e8de42c.png)

ä»å†…å®¹æ¥çœ‹ï¼Œ2B çš„æ¨¡å‹ï¼Œä»¥åŠå¯èƒ½æ˜¯ä¸­æ–‡æ•°æ®è®­ç»ƒä¸å¤Ÿï¼Œç›®å‰å›ç­”çš„æ˜æ˜¾ä¸åƒä¸ª"äºº" ğŸ˜‚ã€‚


## å•æ ·ä¾‹å¾®è°ƒ

```
## ç™»è¿œç¨‹èŠ‚ç‚¹ï¼Œå¼€å§‹è¿è¡Œ
# ç”³è¯·èµ„æºï¼Œä¸€èˆ¬æ ¸å¡æ¯”ä¸º 8:1
whichpartition
  ...
# salloc -p kshdtest -N 1 -n 8 --gres=dcu:1
salloc -p ${partition} -N 1 -n 8 --gres=dcu:1
# æŸ¥çœ‹ä½œä¸šåˆ—è¡¨ï¼Œæ‰¾åˆ°èŠ‚ç‚¹å
squeue
  ...
# ç™»å½•åˆ°èŠ‚ç‚¹ä¸Š
ssh ${node_name}
# ä¸»æŒ‡ä»¤
source load_torch2.1_py3.sh
cd LLM/LLaMA-Factory
./sft_single.sh ../gemma/models/gemma-2b test q_proj,v_proj outputs/sft_output_gemma_20240321
  ...
exit  # ç¬¬ä¸€ä¸ª exitï¼Œç™»å‡ºèŠ‚ç‚¹
exit  # ç¬¬äºŒä¸ª exitï¼Œé€€å‡ºä½œä¸š
```

ä¸ä¹‹å‰ç±»ä¼¼ï¼Œå¾®è°ƒç»“æœæ­£å¸¸ï¼Œè€—æ—¶ä¹ŸåŸºæœ¬ä¸€è‡´ã€‚

## å¾®è°ƒåï¼Œæ¨¡å‹éƒ¨ç½²å’Œæ¨ç†

LoRA å¾®è°ƒåï¼Œé™¤äº†åˆå¹¶æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥ç”¨ peft æ–¹å¼åŠ è½½ã€‚åœ¨ adapter_config.json æ–‡ä»¶ä¸­ï¼Œ<font color='red'>ä¼šç”¨ base_model_name_or_path æŒ‡å‘åŸæ¨¡å‹åœ°å€</font>ã€‚

```
## 1 åœ¨ adapter_config.json æ–‡ä»¶ä¸­ï¼Œç¡®è®¤ base_model_name_or_path è·¯å¾„æ­£ç¡®
# è¯¥è·¯å¾„æ˜¯ç›¸å¯¹äºæœ€ç»ˆæ‰§è¡Œçš„ python è„šæœ¬çš„ä½ç½®ï¼Œä¾‹å¦‚ gemma åŒçº§ç›®å½•ä¸‹

## 2 è°ƒç”¨æ¨¡å‹ï¼Œå®éªŒç»“æœ
# ç”³è¯·èµ„æºï¼Œä¸€èˆ¬æ ¸å¡æ¯”ä¸º 8:1
whichpartition
  ...
# salloc -p kshdtest -N 1 -n 8 --gres=dcu:1
salloc -p ${partition}$ -N 1 -n 8 --gres=dcu:1
# æŸ¥çœ‹ä½œä¸šåˆ—è¡¨ï¼Œæ‰¾åˆ°èŠ‚ç‚¹å
squeue
  ...
# ç™»å½•åˆ°èŠ‚ç‚¹ä¸Š
ssh ${node_name}
# ä¸»æŒ‡ä»¤
source load_torch2.1_py3.sh
cd LLM/gemma
python3 cli_generate.py --seed=0 --prompt=ä½ å¥½ --model_path=../LLaMA-Factory/outputs/sft_output_gemma_20240321 --use_peft
  ...
exit  # ç¬¬ä¸€ä¸ª exitï¼Œç™»å‡ºèŠ‚ç‚¹
exit  # ç¬¬äºŒä¸ª exitï¼Œé€€å‡ºä½œä¸š
```

ç›¸æ¯”äºå¾®è°ƒå‰ï¼Œç­‰å¾… 1-3 åˆ†é’Ÿï¼Œå¼€å§‹æœ‰è¾“å‡ºï¼ŒåŠ è½½æ¨¡å‹ã€‚æ€»å…±çº¦ 3-5 åˆ†é’Ÿï¼Œå®Œæˆè¾“å‡ºã€‚

![](https://files.mdnice.com/user/44560/29d1d971-72d7-43e2-a1f9-c28598107848.png)

ç”±äºæ²¡æœ‰çœŸçš„ç”¨å¤§é‡ç›¸å…³æ•°æ®æå‡æ•ˆæœï¼Œä»¥åŠä½¿ç”¨äº†ç›¸åŒçš„ seed å€¼ï¼Œå› æ­¤å¾—åˆ°äº†ä¸€æ ·çš„å›ç­”ç»“æœã€‚ä½†æ˜¯ä½¿ç”¨ output ç›®å½•ä¸‹çš„å¾®è°ƒåçš„æ¨¡å‹æ–‡ä»¶**æˆåŠŸè¿è¡Œ**ï¼Œè¯´æ˜æ•´ä½“æµç¨‹å·²æ‰“é€š ğŸ˜ã€‚

![](https://files.mdnice.com/user/44560/578ff814-090a-4616-91e8-c7c64fa8a8fb.png)

# LLaMA2

LLaMA2 æ˜¯ **Meta AI** å…¬å¸å‘å¸ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚

## éƒ¨ç½²å’Œæ¨ç†

```
cd LLM
## 1 å·¥ç¨‹ä»£ç å’Œæ¨¡å‹æ–‡ä»¶
git clone https://github.com/meta-llama/llama.git
  ...
cd llama
mkdir models && cd models
# https://huggingface.co/meta-llama/Llama-2-7b-hf
# æ¨¡å‹æ–‡ä»¶éœ€è¦ç”³è¯·ï¼ŒåŸºæœ¬å½“å¤©å¾ˆå¿«é€šï¼›ä¹‹åè¿›è¡Œä¸‹è½½ï¼Œä¸Šä¼ ã€‚
mkdir Llama-2-7b-hf
...

## 2 æ¨¡å‹æ¨ç†ä»£ç ï¼Œå¤åˆ¶ gemma çš„å³å¯ã€‚
cd ..
cp ../gemma/cli_generate.py cli_generate.py

## 3 ç™»è¿œç¨‹èŠ‚ç‚¹ï¼Œå¼€å§‹è¿è¡Œ
# ç”³è¯·èµ„æºï¼Œä¸€èˆ¬æ ¸å¡æ¯”ä¸º 8:1
whichpartition
  ...
# salloc -p kshdtest -N 1 -n 8 --gres=dcu:1
salloc -p ${partition} -N 1 -n 8 --gres=dcu:1
# æŸ¥çœ‹ä½œä¸šåˆ—è¡¨ï¼Œæ‰¾åˆ°èŠ‚ç‚¹å
squeue
  ...
# ç™»å½•åˆ°èŠ‚ç‚¹ä¸Š
ssh ${node_name}
# ä¸»æŒ‡ä»¤
source load_torch2.1_py3.sh
cd LLM/llama
python3 cli_generate.py --seed=0 --prompt=ä½ å¥½ --model_path=./models/Llama-2-7b-hf
  ...
exit  # ç¬¬ä¸€ä¸ª exitï¼Œç™»å‡ºèŠ‚ç‚¹
exit  # ç¬¬äºŒä¸ª exitï¼Œé€€å‡ºä½œä¸š
```

ç±»ä¼¼çš„ï¼Œç­‰å¾… 1-3 åˆ†é’Ÿï¼Œå¼€å§‹æœ‰è¾“å‡ºï¼ŒåŠ è½½æ¨¡å‹ã€‚æ€»å…±çº¦ 3-5 åˆ†é’Ÿï¼Œå®Œæˆè¾“å‡ºã€‚

![](https://files.mdnice.com/user/44560/f442b66a-9691-41d7-9876-0db18586b169.png)

ç›¸æ¯”äº Baichuan2ã€ChatGLM3 ç­‰ä¸­æ–‡å¤§æ¨¡å‹ï¼ŒåŸç”Ÿçš„ LLaMA åœ¨ä¸­æ–‡é—®ç­”æ•ˆæœä¸Šï¼Œç¡®å®ä¸€èˆ¬ ğŸ˜‚ã€‚

## å•æ ·ä¾‹å¾®è°ƒ

```
## ç™»è¿œç¨‹èŠ‚ç‚¹ï¼Œå¼€å§‹è¿è¡Œ
# ç”³è¯·èµ„æºï¼Œä¸€èˆ¬æ ¸å¡æ¯”ä¸º 8:1
whichpartition
  ...
# salloc -p kshdtest -N 1 -n 8 --gres=dcu:1
salloc -p ${partition} -N 1 -n 8 --gres=dcu:1
# æŸ¥çœ‹ä½œä¸šåˆ—è¡¨ï¼Œæ‰¾åˆ°èŠ‚ç‚¹å
squeue
  ...
# ç™»å½•åˆ°èŠ‚ç‚¹ä¸Š
ssh ${node_name}
# ä¸»æŒ‡ä»¤
source load_torch2.1_py3.sh
cd LLM/LLaMA-Factory
./sft_single.sh ../llama/models/Llama-2-7b-hf test q_proj,v_proj outputs/sft_output_llama_20240321
  ...
exit  # ç¬¬ä¸€ä¸ª exitï¼Œç™»å‡ºèŠ‚ç‚¹
exit  # ç¬¬äºŒä¸ª exitï¼Œé€€å‡ºä½œä¸š
```

å¾®è°ƒä¾ç„¶æ˜¯ç¨³å¦‚è€ç‹—ï¼Œç»“æœæ­£å¸¸ï¼Œè€—æ—¶ä¹ŸåŸºæœ¬ä¸€è‡´ã€‚

## å¾®è°ƒåï¼Œæ¨¡å‹éƒ¨ç½²å’Œæ¨ç†

```
## 1 åœ¨ adapter_config.json æ–‡ä»¶ä¸­ï¼Œç¡®è®¤ base_model_name_or_path è·¯å¾„æ­£ç¡®

## 2 è°ƒç”¨æ¨¡å‹ï¼Œå®éªŒç»“æœ
# ç”³è¯·èµ„æºï¼Œä¸€èˆ¬æ ¸å¡æ¯”ä¸º 8:1
whichpartition
  ...
# salloc -p kshdtest -N 1 -n 8 --gres=dcu:1
salloc -p ${partition}$ -N 1 -n 8 --gres=dcu:1
# æŸ¥çœ‹ä½œä¸šåˆ—è¡¨ï¼Œæ‰¾åˆ°èŠ‚ç‚¹å
squeue
  ...
# ç™»å½•åˆ°èŠ‚ç‚¹ä¸Š
ssh ${node_name}
# ä¸»æŒ‡ä»¤
source load_torch2.1_py3.sh
cd LLM/llama
python3 cli_generate.py --seed=0 --prompt=ä½ å¥½ --model_path=../LLaMA-Factory/outputs/sft_output_llama_20240321 --use_peft
  ...
exit  # ç¬¬ä¸€ä¸ª exitï¼Œç™»å‡ºèŠ‚ç‚¹
exit  # ç¬¬äºŒä¸ª exitï¼Œé€€å‡ºä½œä¸š
```

ä¸ Gemma ç±»ä¼¼ï¼Œå¾®è°ƒåçš„æ¨¡å‹å¾—åˆ°äº†ç›¸ä¼¼çš„è¾“å‡ºï¼Œå³å¾®è°ƒæµç¨‹å®Œæˆæ‰“é€š (ã€ƒ'â–½'ã€ƒ)ã€‚

![](https://files.mdnice.com/user/44560/a7f55d48-e00b-4447-962d-14281fe5e9fc.png)


# æ€» ç»“

ä¸»è¦é¢ä¸´çš„å›°éš¾:

- xFormers æš‚æ—¶æ²¡æœ‰è¾ƒå¥½çš„ AMD é€‚é…æ–¹æ¡ˆã€‚

ä¸»è¦è§£å†³æ–¹æ¡ˆ:

- æ›²çº¿æ•‘å›½ï¼Œå°è¯• Baichuan2ã€ChatGLM3ã€Gemmaã€LLaMA2 ç­‰å…¶ä»–å¤§æ¨¡å‹çš„å¾®è°ƒï¼Œæ‰“é€šå•æ ·æœ¬å¾®è°ƒæµç¨‹ã€‚
- ç›®å‰ä¸ºæ­¢ï¼Œ<font color='red'>ChatGLM3ã€Gemma å’Œ LLaMA2 å·²ç»å®Œæ•´æ‰“é€š</font>ï¼Œå¯ä»¥ç”¨æ¥ç»§ç»­æ¢ç´¢ï¼›ä»…æœ‰ Baichuan2 ä¸»è¦å¡åœ¨ xFormers çš„å…¼å®¹æ€§é—®é¢˜ä¸Šã€‚

ä¸‹ä¸€æ­¥è®¡åˆ’:

- å®éªŒå®Œæˆï¼Œå¼€å§‹å®æ“ã€‚
  - åœ¨å…·ä½“<font color='red'>ä¸šåŠ¡åœºæ™¯</font>ä¸­ï¼Œä½¿ç”¨æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œå¹¶éªŒè¯æ•ˆæœã€‚
  - æ—¶é—´ä¼˜åŒ–: åˆ†å¸ƒå¼<font color='red'>å¤šå¡è®­ç»ƒ</font>...
  - <font color='red'>æ•ˆæœå¯¹æ¯”</font>: æ¨¡å‹ã€æ¨¡å‹å‚æ•°è§„æ¨¡ã€æ•°æ®è§„æ¨¡...
- é—ç•™çš„ Baichuan2 æœªèƒ½æ‰“é€šå¾®è°ƒæµç¨‹ï¼Œå¾…æºç é˜¶æ®µå»è§£å†³ã€‚
